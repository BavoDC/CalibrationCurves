@article{VanCalster2020,
issn = {0962-2802},
abstract = {When developing risk prediction models on datasets with limited sample size, shrinkage methods are recommended. Earlier studies showed that shrinkage results in better predictive performance on average. This simulation study aimed to investigate the variability of regression shrinkage on predictive performance for a binary outcome. We compared standard maximum likelihood with the following shrinkage methods: uniform shrinkage (likelihood-based and bootstrap-based), penalized maximum likelihood (ridge) methods, LASSO logistic regression, adaptive LASSO, and Firth’s correction. In the simulation study, we varied the number of predictors and their strength, the correlation between predictors, the event rate of the outcome, and the events per variable. In terms of results, we focused on the calibration slope. The slope indicates whether risk predictions are too extreme (slope < 1) or not extreme enough (slope > 1). The results can be summarized into three main findings. First, shrinkage improved calibration slopes on average. Second, the between-sample variability of calibration slopes was often increased relative to maximum likelihood. In contrast to other shrinkage approaches, Firth’s correction had a small shrinkage effect but showed low variability. Third, the correlation between the estimated shrinkage and the optimal shrinkage to remove overfitting was typically negative, with Firth’s correction as the exception. We conclude that, despite improved performance on average, shrinkage often worked poorly in individual datasets, in particular when it was most needed. The results imply that shrinkage methods do not solve problems associated with small sample size or low number of events per variable.},
journal = {Statistical methods in medical research},
pages = {3166--3178},
volume = {29},
publisher = {SAGE Publications},
number = {11},
year = {2020},
title = {Regression shrinkage methods for clinical prediction models do not guarantee improved performance: Simulation study},
copyright = {The Author(s) 2020},
language = {eng},
address = {London, England},
author = {Van Calster, Ben and van Smeden, Maarten and De Cock, Bavo and Steyerberg, Ewout W},
keywords = {Index Medicus},
}

@article{VanCalster2016,
  title={A calibration hierarchy for risk models was defined: from utopia to empirical data},
  author={Van Calster, Ben and Nieboer, Daan and Vergouwe, Yvonne and De Cock, Bavo and Pencina, Michael J and Steyerberg, Ewout W},
  journal={Journal of Clinical Epidemiology},
  volume={74},
  pages={167--176},
  year={2016},
  publisher={Elsevier}
}

@book{ClinicalPredictionModels,
series = {Statistics for biology and health},
abstract = {Despite advances in statistical approaches towards clinical outcome prediction, these innovations are insufficiently utilized in medical research. This book provides information on how modern statistical concepts and regression methods can be applied.},
publisher = {Springer International Publishing AG},
booktitle = {Clinical Prediction Models},
isbn = {3030163989},
year = {2019},
title = {Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating},
language = {eng},
address = {Cham},
author = {Steyerberg, Ewout W},
keywords = {Evidence-based medicine-Statistical methods ; Medical statistics ; Medicine-Research-Statistical methods},
}

@article{VanCalster2019,
issn = {1741-7015},
abstract = {Background: The assessment of calibration performance of risk prediction models based on regression or more flexible machine learning algorithms receives little attention. Main text: Herein, we argue that this needs to change immediately because poorly calibrated algorithms can be misleading and potentially harmful for clinical decision-making. We summarize how to avoid poor calibration at algorithm development and how to assess calibration at algorithm validation, emphasizing balance between model complexity and the available sample size. At external validation, calibration curves require sufficiently large samples. Algorithm updating should be considered for appropriate support of clinical practice. Conclusion: Efforts are required to avoid poor calibration when developing prediction models, to evaluate calibration when validating models, and to update models when indicated. The ultimate aim is to optimize the utility of predictive analytics for shared decision-making and patient counseling.},
journal = {BMC medicine},
pages = {230--230},
volume = {17},
publisher = {BioMed Central},
number = {1},
year = {2019},
title = {Calibration: The Achilles heel of predictive analytics},
copyright = {info:eu-repo/semantics/openAccess},
language = {eng},
address = {England},
author = {Van Calster, Ben and McLernon, David J and van Smeden, Maarten and Wynants, Laure and Steyerberg, Ewout W and Bossuyt, Patrick and Collins, Gary S and MacAskill, Petra and Moons, Karel G. M and Vickers, Anew J},
keywords = {Predictive Value of Tests ; Algorithms ; Humans ; Middle Aged ; Machine Learning - standards ; Adult ; Male ; Aged ; Calibration - standards ; Index Medicus ; Heterogeneity ; Model performance ; Opinion ; Risk prediction models ; Calibration ; Predictive analytics ; Overfitting},
organization = {Topic Group ‘Evaluating diagnostic tests and prediction models’ of the STRATOS initiative},
}

@book{GLM,
series = {Monographs on statistics and applied probability 37},
publisher = {Chapman and Hall},
isbn = {0412317605},
year = {1999},
title = {Generalized linear models},
edition = {2nd ed. repr.},
address = {London},
author = {McCullagh, P and Nelder, J. A},
keywords = {Theory of economic-mathematical models},
}

@article{Christodoulou2019,
issn = {0895-4356},
abstract = {The objective of this study was to compare performance of logistic regression (LR) with machine learning (ML) for clinical prediction modeling in the literature.
We conducted a Medline literature search (1/2016 to 8/2017) and extracted comparisons between LR and ML models for binary outcomes.
We included 71 of 927 studies. The median sample size was 1,250 (range 72–3,994,872), with 19 predictors considered (range 5–563) and eight events per predictor (range 0.3–6,697). The most common ML methods were classification trees, random forests, artificial neural networks, and support vector machines. In 48 (68\%) studies, we observed potential bias in the validation procedures. Sixty-four (90\%) studies used the area under the receiver operating characteristic curve (AUC) to assess discrimination. Calibration was not addressed in 56 (79\%) studies. We identified 282 comparisons between an LR and ML model (AUC range, 0.52–0.99). For 145 comparisons at low risk of bias, the difference in logit(AUC) between LR and ML was 0.00 (95\% confidence interval, −0.18 to 0.18). For 137 comparisons at high risk of bias, logit(AUC) was 0.34 (0.20–0.47) higher for ML.
We found no evidence of superior performance of ML over LR. Improvements in methodology and reporting are needed for studies that compare modeling algorithms.},
journal = {Journal of clinical epidemiology},
pages = {12--22},
volume = {110},
publisher = {Elsevier Inc},
year = {2019},
title = {A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models},
copyright = {2019 Elsevier Inc.},
language = {eng},
address = {NEW YORK},
author = {Christodoulou, Evangelia and Ma, Jie and Collins, Gary S and Steyerberg, Ewout W and Verbakel, Jan Y and Van Calster, Ben},
keywords = {Clinical prediction models ; Logistic regression ; Calibration ; Reporting ; Machine learning ; AUC ; Health Care Sciences & Services ; Public, Environmental & Occupational Health ; Life Sciences & Biomedicine ; Science & Technology ; Models, Theoretical ; Predictive Value of Tests ; Algorithms ; Area Under Curve ; Humans ; Sensitivity and Specificity ; Logistic Models ; Outcome Assessment, Health Care ; Supervised Machine Learning ; Index Medicus},
}


@article{ADNEXmodel,
issn = {0959-8138},
abstract = {Objectives To develop a risk prediction model to preoperatively discriminate between benign, borderline, stage I invasive, stage II-IV invasive, and secondary metastatic ovarian tumours.Design Observational diagnostic study using prospectively collected clinical and ultrasound data.Setting 24 ultrasound centres in 10 countries.Participants Women with an ovarian (including para-ovarian and tubal) mass and who underwent a standardised ultrasound examination before surgery. The model was developed on 3506 patients recruited between 1999 and 2007, temporally validated on 2403 patients recruited between 2009 and 2012, and then updated on all 5909 patients.Main outcome measures Histological classification and surgical staging of the mass.Results The Assessment of Different NEoplasias in the adneXa (ADNEX) model contains three clinical and six ultrasound predictors: age, serum CA-125 level, type of centre (oncology centres v other hospitals), maximum diameter of lesion, proportion of solid tissue, more than 10 cyst locules, number of papillary projections, acoustic shadows, and ascites. The area under the receiver operating characteristic curve (AUC) for the classic discrimination between benign and malignant tumours was 0.94 (0.93 to 0.95) on temporal validation. The AUC was 0.85 for benign versus borderline, 0.92 for benign versus stage I cancer, 0.99 for benign versus stage II-IV cancer, and 0.95 for benign versus secondary metastatic. AUCs between malignant subtypes varied between 0.71 and 0.95, with an AUC of 0.75 for borderline versus stage I cancer and 0.82 for stage II-IV versus secondary metastatic. Calibration curves showed that the estimated risks were accurate.Conclusions The ADNEX model discriminates well between benign and malignant tumours and offers fair to excellent discrimination between four types of ovarian malignancy. The use of ADNEX has the potential to improve triage and management decisions and so reduce morbidity and mortality associated with adnexal pathology.},
journal = {BMJ : British Medical Journal},
pages = {g5920--g5920},
volume = {349},
publisher = {British Medical Journal Publishing Group},
number = {oct07 3},
year = {2014},
title = {Evaluating the risk of ovarian cancer before surgery using the ADNEX model to differentiate between benign, borderline, early and advanced stage invasive, and secondary metastatic tumours: prospective multicentre diagnostic study},
copyright = {Van Calster et al 2014},
language = {eng},
address = {England},
author = {Van Calster, Ben and Van Hoorde, Kirsten and Valentin, Lil and Testa, Antonia C and Fischerova, Daniela and Van Holsbeke, Caroline and Savelli, Luca and Franchi, Dorella and Epstein, Elisabeth and Kaijser, Jeroen and Van Belle, Vanya and Czekierdowski, Artur and Guerriero, Stefano and Fruscio, Robert and Lanzani, Chiara and Scala, Felice and Bourne, Tom and Timmerman, Dirk},
keywords = {RESEARCH ; Predictive Value of Tests ; Risk Assessment - methods ; Prospective Studies ; Humans ; Ovarian Neoplasms - pathology ; Models, Statistical ; Ovarian Neoplasms - diagnostic imaging ; Adnexal Diseases - pathology ; Ultrasonography ; Adnexal Diseases - diagnostic imaging ; Adult ; Female ; Neoplasm Staging ; Index Medicus ; Abridged Index Medicus ; Clinical Medicine ; Obstetrics, Gynecology and Reproductive Medicine ; Reproduktionsmedicin och gynekologi ; Medical and Health Sciences ; Klinisk medicin ; Medicin och hälsovetenskap},
organization = {International Ovarian Tumour Analysis Group},
}

@article{Alba2017,
issn = {0098-7484},
abstract = {Accurate information regarding prognosis is fundamental to optimal clinical care. The best approach to assess patient prognosis relies on prediction models that simultaneously consider a number of prognostic factors and provide an estimate of patients’ absolute risk of an event. Such prediction models should be characterized by adequately discriminating between patients who will have an event and those who will not and by adequate calibration ensuring accurate prediction of absolute risk. This Users’ Guide will help clinicians understand the available metrics for assessing discrimination, calibration, and the relative performance of different prediction models. This article complements existing Users’ Guides that address the development and validation of prediction models. Together, these guides will help clinicians to make optimal use of existing prediction models.},
journal = {JAMA : the journal of the American Medical Association},
pages = {1377--1384},
volume = {318},
publisher = {American Medical Association},
number = {14},
year = {2017},
title = {{Discrimination and Calibration of Clinical Prediction Models: Users' Guides to the Medical Literature}},
language = {eng},
address = {United States},
author = {Alba, Ana Carolina and Agoritsas, Thomas and Walsh, Michael and Hanna, Steven and Iorio, Alfonso and Devereaux, P. J and McGinn, Thomas and Guyatt, Gordon},
keywords = {Prognosis ; Risk Assessment ; Area Under Curve ; Humans ; Sensitivity and Specificity ; ROC Curve ; Models, Statistical ; Medicine ; Medical literature ; Analysis ; Aged patients ; Research ; History ; Health aspects ; Tobacco habit ; Index Medicus ; Abridged Index Medicus},
}


@misc{Campo2023GCF,
      title={Towards reliable predictive analytics: a generalized calibration framework},
      author={Bavo {De Cock Campo}},
      year={2023},
      eprint={2309.08559},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}


@article{ClusteredCalibration,
  title={Clustered Flexible Calibration Plots For Binary Outcomes Using Random Effects Modeling},
  author={Barre{\~n}ada, Lasai and Campo, Bavo DC and Wynants, Laure and Van Calster, Ben},
  journal={arXiv preprint arXiv:2503.08389},
  year={2025}
}
